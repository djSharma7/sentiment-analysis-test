{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"sentiment_2.ipynb","provenance":[{"file_id":"19UtQCSqCCSIFcMoeCBAglTlXg_ZofhPh","timestamp":1576307547872}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"542a67dd1a15464393ca9fe1f8160fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_a48fbf214002420da8e01438a06afb95","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_32221eb205574f41aa4f5ce482dc66c3","IPY_MODEL_0fa2514c0d6642b7af031ed9adcc0eb3"]}},"a48fbf214002420da8e01438a06afb95":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"32221eb205574f41aa4f5ce482dc66c3":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_603f524167ea4d2f8e816b2f8a1e19d1","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":313,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":313,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92e8b14f38064f8a8491f93d941940d0"}},"0fa2514c0d6642b7af031ed9adcc0eb3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_448640d132624108858738c6344f4edb","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 313/313 [00:00&lt;00:00, 8.47kB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9f73ecf4052b4409a7df05092c9d72b0"}},"603f524167ea4d2f8e816b2f8a1e19d1":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"92e8b14f38064f8a8491f93d941940d0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"448640d132624108858738c6344f4edb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9f73ecf4052b4409a7df05092c9d72b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"64707e538c854752b6caade64ea7a57c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_aaa3eae9e8e44697ae35f9733725f745","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc6a5be1f2d64c6ea55d751163b54afe","IPY_MODEL_bc9fcc75238e42fbbb4a5450f6f9e42f"]}},"aaa3eae9e8e44697ae35f9733725f745":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc6a5be1f2d64c6ea55d751163b54afe":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c3229d513adc4dbe8b7008260b279bd7","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8ef670da27c341139e4ffe6a10f230a6"}},"bc9fcc75238e42fbbb4a5450f6f9e42f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2f064fc030f5499aae7e91bd56ed0d6d","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 232k/232k [00:00&lt;00:00, 1.23MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_16930e644d0244bfa7b7b52070263637"}},"c3229d513adc4dbe8b7008260b279bd7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8ef670da27c341139e4ffe6a10f230a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2f064fc030f5499aae7e91bd56ed0d6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"16930e644d0244bfa7b7b52070263637":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0144d43bb0784a3da4ea917ed8d1f984":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_877f293ce6234b92b20dc48627e0eb75","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_08399da8cc024e04848f20494fa8757d","IPY_MODEL_cc33468a0fed445aa994fd343911500f"]}},"877f293ce6234b92b20dc48627e0eb75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"08399da8cc024e04848f20494fa8757d":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_33274cba426341749465ead1d22c4c3e","_dom_classes":[],"description":"Downloading","_model_name":"IntProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_00bb1d291492418abe12542bb5006a03"}},"cc33468a0fed445aa994fd343911500f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8161934b38864ce0aedd10db54814f0c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 440M/440M [00:10&lt;00:00, 40.8MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_794bb611fc0141bfb232bdd46c021099"}},"33274cba426341749465ead1d22c4c3e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"00bb1d291492418abe12542bb5006a03":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8161934b38864ce0aedd10db54814f0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"794bb611fc0141bfb232bdd46c021099":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0ba50851230647b29cd4f72ae61fef83":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d999f8bca31b4aeea83fb7bd2b3d69d4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cc86a2e12d574b55b67c2849b5b11a42","IPY_MODEL_8d728ee83f39456a9623d7e959223d9e"]}},"d999f8bca31b4aeea83fb7bd2b3d69d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"cc86a2e12d574b55b67c2849b5b11a42":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_a2283b0e19924b8d8c986171007825ef","_dom_classes":[],"description":"Iteration","_model_name":"IntProgressModel","bar_style":"success","max":299,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":299,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a818c89914d54959bb1b5c0933013c47"}},"8d728ee83f39456a9623d7e959223d9e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_593a332db5764a33882570e9d817e7ff","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 299/299 [00:45&lt;00:00,  6.63it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6e18bbd7495b49b89a14859c40b22c5e"}},"a2283b0e19924b8d8c986171007825ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a818c89914d54959bb1b5c0933013c47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"593a332db5764a33882570e9d817e7ff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6e18bbd7495b49b89a14859c40b22c5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"96c74f88edbe44628e6e45b9c7ce5e9f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_4388c75f99e344549d42321587bedd35","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_52a6c0bcb374469e8eec878ad7f62391","IPY_MODEL_2cf42c60318441e5b96b109bb0e8c4aa"]}},"4388c75f99e344549d42321587bedd35":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"52a6c0bcb374469e8eec878ad7f62391":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_425e4cfcbc134458a19859983743e2ef","_dom_classes":[],"description":"Evaluating","_model_name":"IntProgressModel","bar_style":"success","max":75,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":75,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a2a308d5b08c458ca1764fa80260e4b2"}},"2cf42c60318441e5b96b109bb0e8c4aa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a1274cb10f004fdd97b0946f63cbd7e0","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 75/75 [00:02&lt;00:00, 25.84it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_060bfc99786445f6ac1fef29cf388dc1"}},"425e4cfcbc134458a19859983743e2ef":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a2a308d5b08c458ca1764fa80260e4b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a1274cb10f004fdd97b0946f63cbd7e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"060bfc99786445f6ac1fef29cf388dc1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a70e530c639c4bf38d3dfb5166d0590a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_f42745012b1c48bfb674483be7f1f734","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0eddacfe6d3b484381af89786d49e6c9","IPY_MODEL_cd9a4b528a0143979673856258fe6ce1"]}},"f42745012b1c48bfb674483be7f1f734":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0eddacfe6d3b484381af89786d49e6c9":{"model_module":"@jupyter-widgets/controls","model_name":"IntProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_fc07f6d71c6d462d93360bd01521628c","_dom_classes":[],"description":"Evaluating","_model_name":"IntProgressModel","bar_style":"success","max":75,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":75,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_68c87097ec5b44f0ae72128e01759235"}},"cd9a4b528a0143979673856258fe6ce1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f8730c0548944b65b9ec93a5e1d37ffc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100% 75/75 [00:02&lt;00:00, 25.39it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_5cb00c9beaab4e6694f30a254a45eb72"}},"fc07f6d71c6d462d93360bd01521628c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"68c87097ec5b44f0ae72128e01759235":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f8730c0548944b65b9ec93a5e1d37ffc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"5cb00c9beaab4e6694f30a254a45eb72":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"VeXuXWylz7BD","colab":{"base_uri":"https://localhost:8080/","height":655},"outputId":"6a00343f-558c-4942-c63d-e34ce33d3699","executionInfo":{"status":"ok","timestamp":1576307745697,"user_tz":-330,"elapsed":18745,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}}},"source":["!pip install transformers"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting transformers\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl (387kB)\n","\u001b[K     |████████████████████████████████| 389kB 4.8MB/s \n","\u001b[?25hCollecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/3d/efb655a670b98f62ec32d66954e1109f403db4d937c50d779a75b9763a29/sentencepiece-0.1.83-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 51.1MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.21.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers) (4.28.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1f/8e/ed5364a06a9ba720fddd9820155cc57300d28f5f43a6fd7b7e817177e642/sacremoses-0.0.35.tar.gz (859kB)\n","\u001b[K     |████████████████████████████████| 860kB 49.0MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers) (1.10.36)\n","Collecting regex\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/db/4b29a0adec5881542cd81cb5d1929b5c0787003c5740b3c921e627d9c2e5/regex-2019.12.9.tar.gz (669kB)\n","\u001b[K     |████████████████████████████████| 675kB 54.0MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.17.4)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.8)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.12.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.14.1)\n","Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.2.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (0.9.4)\n","Requirement already satisfied: botocore<1.14.0,>=1.13.36 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers) (1.13.36)\n","Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (2.6.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.36->boto3->transformers) (0.15.2)\n","Building wheels for collected packages: sacremoses, regex\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.35-cp36-none-any.whl size=883999 sha256=b43dc47700c818db57c7540a1528ae6662a47b53b93ec13674704e0cdae01316\n","  Stored in directory: /root/.cache/pip/wheels/63/2a/db/63e2909042c634ef551d0d9ac825b2b0b32dede4a6d87ddc94\n","  Building wheel for regex (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for regex: filename=regex-2019.12.9-cp36-cp36m-linux_x86_64.whl size=609177 sha256=54d2af53ecef1abdd443f919ade1239823bd5feba42b0cd3ef11713389afb9f8\n","  Stored in directory: /root/.cache/pip/wheels/0d/fb/b3/a89169557229468c49ca64f6839418f22461f6ee0a74f342b1\n","Successfully built sacremoses regex\n","Installing collected packages: sentencepiece, sacremoses, regex, transformers\n","Successfully installed regex-2019.12.9 sacremoses-0.0.35 sentencepiece-0.1.83 transformers-2.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5VvJLKhovUDq","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rrWMifGWvnka","colab_type":"code","outputId":"3c532618-ea05-417c-a8fc-7f01b100a856","executionInfo":{"status":"ok","timestamp":1576308382853,"user_tz":-330,"elapsed":1332,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["from google.colab import drive\n","drive.mount('/gdrive')\n","%cd /gdrive/My Drive/TCS/sentiment-analysis/type"],"execution_count":20,"outputs":[{"output_type":"stream","text":["Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n","/gdrive/My Drive/TCS/sentiment-analysis/type\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"j7ZAgbMywAG9","colab_type":"code","outputId":"cc072614-c7fd-430d-fe49-4f27b5a1c400","executionInfo":{"status":"ok","timestamp":1576307812070,"user_tz":-330,"elapsed":3581,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["ls"],"execution_count":3,"outputs":[{"output_type":"stream","text":["sentiment_2.ipynb  test.ipynb  utils.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"u4XFAZHwvQoe","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import glob\n","import logging\n","import os\n","import random\n","import json\n","\n","import numpy as np\n","import torch\n","from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n","                              TensorDataset)\n","import random\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm_notebook, trange\n","\n","\n","from transformers import (WEIGHTS_NAME, BertConfig, BertTokenizer, BertForSequenceClassification,\n","                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n","                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n","                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n","\n","from transformers import BertModel, BertPreTrainedModel\n","from torch import nn\n","from torch.nn import MSELoss, CrossEntropyLoss\n","\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup as WarmupLinearSchedule\n","\n","from utils import (convert_examples_to_features,\n","                        output_modes, processors, InputExample, convert_example_to_feature)\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aH-ucvqjdUP-","colab":{}},"source":["# class BertForSequenceClassification(BertPreTrainedModel):\n","#     def __init__(self, config):\n","#         super(BertForSequenceClassification, self).__init__(config)\n","#         self.num_labels = config.num_labels\n","\n","#         self.bert = BertModel(config)\n","#         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","#         self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n","#         self.softmax = nn.Softmax(dim=1)\n","\n","#         self.init_weights()\n","\n","#     def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None):\n","\n","#         outputs = self.bert(input_ids,\n","#                             attention_mask=attention_mask,\n","#                             token_type_ids=token_type_ids)\n","\n","#         pooled_output = outputs[1]\n","\n","#         pooled_output = self.dropout(pooled_output)\n","#         logits = self.classifier(pooled_output)\n","#         logits = self.softmax(logits)\n","\n","#         outputs = (logits,) + outputs[2:]  # add hidden states and attention if they are here\n","\n","#         if labels is not None:\n","#             if self.num_labels == 1:\n","#                 #  We are doing regression\n","#                 loss_fct = MSELoss()\n","#                 loss = loss_fct(logits.view(-1), labels.view(-1))\n","#             else:\n","#                 loss_fct = CrossEntropyLoss()\n","#                 loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","#             outputs = (loss,) + outputs\n","\n","#         return outputs  # (loss), logits, (hidden_states), (attentions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"F93_pIopz7BG","colab":{}},"source":["args = {\n","    'data_dir': 'data/',\n","    'model_type':  'bert',\n","    'model_name': 'bert-base-uncased',\n","    'task_name': 'binary',\n","    'output_dir': 'outputs/',\n","    'cache_dir': 'cache/',\n","    'do_train': True,\n","    'do_eval': True,\n","    'fp16': False,\n","    'fp16_opt_level': 'O1',\n","    'max_seq_length': 128,\n","    'output_mode': 'classification',\n","    'train_batch_size': 8,\n","    'eval_batch_size': 8,\n","\n","    'gradient_accumulation_steps': 1,\n","    'num_train_epochs': 1,\n","    'weight_decay': 0,\n","    'learning_rate': 4e-5,\n","    'adam_epsilon': 1e-8,\n","    'warmup_steps': 0,\n","    'max_grad_norm': 1.0,\n","\n","    'logging_steps': 50,\n","    'evaluate_during_training': False,\n","    'save_steps': 2000,\n","    'eval_all_checkpoints': True,\n","\n","    'overwrite_output_dir': False,\n","    'reprocess_input_data': False,\n","    'notes': 'Using Yelp Reviews dataset',\n","    'num_labels': 4\n","}\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"atGwIw3iz7BJ","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Uzr2RwGLz7BL","colab":{}},"source":["with open('args.json', 'w') as f:\n","    json.dump(args, f, indent=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ymjmIyOhz7BN","colab":{}},"source":["if os.path.exists(args['output_dir']) and os.listdir(args['output_dir']) and args['do_train'] and not args['overwrite_output_dir']:\n","    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args['output_dir']))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LAHYiiLMz7BP","colab":{}},"source":["MODEL_CLASSES = {\n","    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n","    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n","    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n","    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n","}\n","\n","config_class, model_class, tokenizer_class = MODEL_CLASSES[args['model_type']]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qm5AguwFz7BR","outputId":"d8e72b35-7d39-431d-fad2-e7e102a3da49","executionInfo":{"status":"ok","timestamp":1576307931382,"user_tz":-330,"elapsed":2526,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":686,"referenced_widgets":["542a67dd1a15464393ca9fe1f8160fc5","a48fbf214002420da8e01438a06afb95","32221eb205574f41aa4f5ce482dc66c3","0fa2514c0d6642b7af031ed9adcc0eb3","603f524167ea4d2f8e816b2f8a1e19d1","92e8b14f38064f8a8491f93d941940d0","448640d132624108858738c6344f4edb","9f73ecf4052b4409a7df05092c9d72b0","64707e538c854752b6caade64ea7a57c","aaa3eae9e8e44697ae35f9733725f745","cc6a5be1f2d64c6ea55d751163b54afe","bc9fcc75238e42fbbb4a5450f6f9e42f","c3229d513adc4dbe8b7008260b279bd7","8ef670da27c341139e4ffe6a10f230a6","2f064fc030f5499aae7e91bd56ed0d6d","16930e644d0244bfa7b7b52070263637"]}},"source":["config = config_class.from_pretrained(args['model_name'], num_labels=4, finetuning_task=args['task_name'])\n","tokenizer = tokenizer_class.from_pretrained(args['model_name'])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmp2dv24wd7\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"542a67dd1a15464393ca9fe1f8160fc5","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=313, style=ProgressStyle(description_width=…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:copying /tmp/tmp2dv24wd7 to cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","INFO:transformers.file_utils:removing temp file /tmp/tmp2dv24wd7\n","INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","INFO:transformers.configuration_utils:Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": \"binary\",\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 4,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmpj48mvpzf\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"64707e538c854752b6caade64ea7a57c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=231508, style=ProgressStyle(description_wid…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:copying /tmp/tmpj48mvpzf to cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n","INFO:transformers.file_utils:removing temp file /tmp/tmpj48mvpzf\n","INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"IGZHNvKAz7BU","outputId":"da8f42a3-ba2e-4bb3-9b46-3761280fee21","scrolled":true,"executionInfo":{"status":"ok","timestamp":1576308071579,"user_tz":-330,"elapsed":17499,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":605,"referenced_widgets":["0144d43bb0784a3da4ea917ed8d1f984","877f293ce6234b92b20dc48627e0eb75","08399da8cc024e04848f20494fa8757d","cc33468a0fed445aa994fd343911500f","33274cba426341749465ead1d22c4c3e","00bb1d291492418abe12542bb5006a03","8161934b38864ce0aedd10db54814f0c","794bb611fc0141bfb232bdd46c021099"]}},"source":["model = model_class.from_pretrained(args['model_name'], num_labels=4)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.bf3b9ea126d8c0001ee8a1e8b92229871d06d36d8808208cc2449280da87785c\n","INFO:transformers.configuration_utils:Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 4,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n","INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpbbgodewc\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0144d43bb0784a3da4ea917ed8d1f984","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Downloading', max=440473133, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:transformers.file_utils:copying /tmp/tmpbbgodewc to cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:transformers.file_utils:creating metadata file for /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","INFO:transformers.file_utils:removing temp file /tmp/tmpbbgodewc\n","INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n","INFO:transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n","INFO:transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xyxKpk_6z7BW","colab":{}},"source":["model.to(device);"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sNIFm9epnT7M","outputId":"d38c63b9-340a-40c7-e9c0-a1e96164472f","executionInfo":{"status":"ok","timestamp":1576308085679,"user_tz":-330,"elapsed":6484,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bsPmRyGE8GnR","outputId":"9cc468ab-16c9-40cb-b903-7e74bac3db3e","executionInfo":{"status":"ok","timestamp":1576308093253,"user_tz":-330,"elapsed":1342,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":33}},"source":["device"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Xe4P94Bfz7Ba","colab":{}},"source":["task = args['task_name']\n","\n","processor = processors[task]()\n","label_list = processor.get_labels()\n","num_labels = len(label_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"abSglfekSOX3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"dfdc7eeb-c784-486e-e03a-19c31f6e4211","executionInfo":{"status":"ok","timestamp":1576308115363,"user_tz":-330,"elapsed":1200,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}}},"source":[""],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['Delivery', 'Packaging', 'Price', 'Product']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xqr_fwM3z7Bd","colab":{}},"source":["def load_and_cache_examples(task, tokenizer, evaluate=False, undersample_scale_factor=0.01):\n","    processor = processors[task]()\n","    output_mode = args['output_mode']\n","    \n","    mode = 'dev' if evaluate else 'train'\n","    cached_features_file = os.path.join(args['data_dir'], f\"cached_{mode}_{args['model_name']}_{args['max_seq_length']}_{task}\")\n","    \n","    if os.path.exists(cached_features_file) and not args['reprocess_input_data']:\n","        logger.info(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","               \n","    else:\n","        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n","        label_list = processor.get_labels()\n","        examples = processor.get_dev_examples(args['data_dir']) if evaluate else processor.get_train_examples(args['data_dir'])\n","        print(\"length of dev set: \", len(examples))\n","#         examples  = [example for example in examples if np.random.rand() < undersample_scale_factor]\n","        print(\"length of dev set: \", len(examples))\n","        \n","        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n","            cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n","            cls_token=tokenizer.cls_token,\n","            sep_token=tokenizer.sep_token,\n","            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n","            pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n","            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n","            process_count=2)\n","        \n","        logger.info(\"Saving features into cached file %s\", cached_features_file)\n","        torch.save(features, cached_features_file)\n","        \n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","    if output_mode == \"classification\":\n","        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n","    elif output_mode == \"regression\":\n","        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n","\n","    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n","    print(\"length of final dataset: \", len(dataset))\n","    return dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"oCul6vvCz7Bg","colab":{}},"source":["def train(train_dataset, model, tokenizer):\n","    train_sampler = RandomSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n","    \n","    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n","    \n","    no_decay = ['bias', 'LayerNorm.weight']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n","    # scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n","    scheduler = WarmupLinearSchedule(optimizer, args['warmup_steps'], t_total)\n","    \n","    if args['fp16']:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n","        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n","        \n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n","    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n","    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n","    \n","    for _ in train_iterator:\n","        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            batch = tuple(t.to(device) for t in batch)\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n","                      'labels':         batch[3]}\n","            outputs = model(**inputs)\n","            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n","            print(\"\\r%f\" % loss, end='')\n","\n","            if args['gradient_accumulation_steps'] > 1:\n","                loss = loss / args['gradient_accumulation_steps']\n","\n","            if args['fp16']:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n","                \n","            else:\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n","                scheduler.step()  # Update learning rate schedule\n","                optimizer.step()\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n","                    # Log metrics\n","                    if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results = evaluate(model, tokenizer)\n","\n","                    logging_loss = tr_loss\n","\n","                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args['output_dir'], 'checkpoint-{}'.format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","        \n","        evaluate(model, tokenizer)\n","\n","\n","    return global_step, tr_loss / global_step"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tUvkEBZUz7Bk","colab":{}},"source":["from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, accuracy_score\n","from scipy.stats import pearsonr\n","\n","def get_mismatched(labels, preds):\n","    mismatched = labels != preds\n","    examples = processor.get_dev_examples(args['data_dir'])\n","    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n","    \n","    return wrong\n","\n","def get_eval_report(labels, preds):\n","    mcc = matthews_corrcoef(labels, preds)\n","    # tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n","    acc = accuracy_score(labels, preds)\n","    return {\n","        \"acc\":acc\n","#         \"mcc\": mcc,\n","#         \"tp\": tp,\n","#         \"tn\": tn,\n","#         \"fp\": fp,\n","#         \"fn\": fn\n","    }, get_mismatched(labels, preds)\n","\n","def compute_metrics(task_name, preds, labels):\n","    assert len(preds) == len(labels)\n","    return get_eval_report(labels, preds)\n","\n","def evaluate(model, tokenizer, prefix=\"\"):\n","    # Loop to handle MNLI double evaluation (matched, mis-matched)\n","    eval_output_dir = args['output_dir']\n","\n","    results = {}\n","    EVAL_TASK = args['task_name']\n","    print(\"eval task: \", EVAL_TASK)\n","\n","    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, evaluate=True)\n","    if not os.path.exists(eval_output_dir):\n","        os.makedirs(eval_output_dir)\n","    print(\"len eval data: \", len(eval_dataset))\n","\n","\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n","        model.eval()\n","        batch = tuple(t.to(device) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {'input_ids':      batch[0],\n","                      'attention_mask': batch[1],\n","                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n","                      'labels':         batch[3]}\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            eval_loss += tmp_eval_loss.mean().item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs['labels'].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    if args['output_mode'] == \"classification\":\n","        preds = np.argmax(preds, axis=1)\n","    elif args['output_mode'] == \"regression\":\n","        preds = np.squeeze(preds)\n","    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n","    results.update(result)\n","\n","    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n","    with open(output_eval_file, \"w\") as writer:\n","        logger.info(\"***** Eval results {} *****\".format(prefix))\n","        for key in sorted(result.keys()):\n","            logger.info(\"  %s = %s\", key, str(result[key]))\n","            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n","\n","    return results, wrong"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"MlaeXY9sz7Bm","outputId":"18c01e77-6b72-431f-aa04-a30afccd129e","executionInfo":{"status":"ok","timestamp":1576308578822,"user_tz":-330,"elapsed":50132,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":507,"referenced_widgets":["0ba50851230647b29cd4f72ae61fef83","d999f8bca31b4aeea83fb7bd2b3d69d4","cc86a2e12d574b55b67c2849b5b11a42","8d728ee83f39456a9623d7e959223d9e","a2283b0e19924b8d8c986171007825ef","a818c89914d54959bb1b5c0933013c47","593a332db5764a33882570e9d817e7ff","6e18bbd7495b49b89a14859c40b22c5e","96c74f88edbe44628e6e45b9c7ce5e9f","4388c75f99e344549d42321587bedd35","52a6c0bcb374469e8eec878ad7f62391","2cf42c60318441e5b96b109bb0e8c4aa","425e4cfcbc134458a19859983743e2ef","a2a308d5b08c458ca1764fa80260e4b2","a1274cb10f004fdd97b0946f63cbd7e0","060bfc99786445f6ac1fef29cf388dc1"]}},"source":["# IMPORTANT #\n","# Due to the 12 hour limit on Google Colab and the time it would take to convert the dataset into features, the load_and_cache_examples() function has been modified\n","# to randomly undersample the dataset by a scale of 0.1\n","\n","if args['do_train']:\n","    train_dataset = load_and_cache_examples(task, tokenizer, undersample_scale_factor=0.1)\n","    global_step, tr_loss = train(train_dataset, model, tokenizer)\n","    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["INFO:__main__:Loading features from cached file data/cached_train_bert-base-uncased_128_binary\n","INFO:__main__:***** Running training *****\n","INFO:__main__:  Num examples = 2386\n","INFO:__main__:  Num Epochs = 1\n","INFO:__main__:  Total train batch size  = 8\n","INFO:__main__:  Gradient Accumulation steps = 1\n","INFO:__main__:  Total optimization steps = 299\n","\n","Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["length of final dataset:  2386\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ba50851230647b29cd4f72ae61fef83","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Iteration', max=299, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r0.328924"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:100: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule.See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["0.079206"],"name":"stdout"},{"output_type":"stream","text":["INFO:__main__:Loading features from cached file data/cached_dev_bert-base-uncased_128_binary\n"],"name":"stderr"},{"output_type":"stream","text":["\r2.591802\n","eval task:  binary\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:__main__:***** Running evaluation  *****\n","INFO:__main__:  Num examples = 597\n","INFO:__main__:  Batch size = 8\n"],"name":"stderr"},{"output_type":"stream","text":["length of final dataset:  597\n","len eval data:  597\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"96c74f88edbe44628e6e45b9c7ce5e9f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Evaluating', max=75, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:__main__:***** Eval results  *****\n","INFO:__main__:  acc = 0.9045226130653267\n","\n","Epoch: 100%|██████████| 1/1 [00:48<00:00, 48.03s/it]\u001b[A\n","\u001b[AINFO:__main__: global_step = 299, average loss = 0.2857229800826331\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"1On6YjIULf7v","outputId":"2790702e-538a-4213-ebad-f097d94686ce","executionInfo":{"status":"ok","timestamp":1576308594748,"user_tz":-330,"elapsed":2691,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}},"colab":{"base_uri":"https://localhost:8080/","height":66}},"source":["if args['do_train']:\n","    if not os.path.exists(args['output_dir']):\n","            os.makedirs(args['output_dir'])\n","    logger.info(\"Saving model checkpoint to %s\", args['output_dir'])\n","    \n","    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n","    model_to_save.save_pretrained(args['output_dir'])\n","    tokenizer.save_pretrained(args['output_dir'])\n","    torch.save(args, os.path.join(args['output_dir'], 'training_args.bin'))\n"],"execution_count":27,"outputs":[{"output_type":"stream","text":["INFO:__main__:Saving model checkpoint to outputs/\n","INFO:transformers.configuration_utils:Configuration saved in outputs/config.json\n","INFO:transformers.modeling_utils:Model weights saved in outputs/pytorch_model.bin\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tqiWWPA0z7Bo","colab":{"base_uri":"https://localhost:8080/","height":650,"referenced_widgets":["a70e530c639c4bf38d3dfb5166d0590a","f42745012b1c48bfb674483be7f1f734","0eddacfe6d3b484381af89786d49e6c9","cd9a4b528a0143979673856258fe6ce1","fc07f6d71c6d462d93360bd01521628c","68c87097ec5b44f0ae72128e01759235","f8730c0548944b65b9ec93a5e1d37ffc","5cb00c9beaab4e6694f30a254a45eb72"]},"outputId":"8a39437e-660b-4c20-e853-c14e01a89cba","executionInfo":{"status":"ok","timestamp":1576308606556,"user_tz":-330,"elapsed":7068,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}}},"source":["results = {}\n","if args['do_eval']:\n","    checkpoints = [args['output_dir']]\n","    if args['eval_all_checkpoints']:\n","        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args['output_dir'] + '/**/' + WEIGHTS_NAME, recursive=True)))\n","        logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n","    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","    print(\"checkpoints: \", checkpoints)\n","\n","    for checkpoint in checkpoints:\n","        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n","        model = model_class.from_pretrained(checkpoint)\n","        model.to(device)\n","        result, wrong_preds = evaluate(model, tokenizer, prefix=global_step)\n","        result = dict((k + '_{}'.format(global_step), v) for k, v in result.items())\n","        results.update(result)\n"],"execution_count":28,"outputs":[{"output_type":"stream","text":["INFO:__main__:Evaluate the following checkpoints: ['outputs']\n","INFO:transformers.configuration_utils:loading configuration file outputs/config.json\n","INFO:transformers.configuration_utils:Model config {\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"finetuning_task\": null,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"is_decoder\": false,\n","  \"layer_norm_eps\": 1e-12,\n","  \"max_position_embeddings\": 512,\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"num_labels\": 4,\n","  \"output_attentions\": false,\n","  \"output_hidden_states\": false,\n","  \"output_past\": true,\n","  \"pruned_heads\": {},\n","  \"torchscript\": false,\n","  \"type_vocab_size\": 2,\n","  \"use_bfloat16\": false,\n","  \"vocab_size\": 30522\n","}\n","\n"],"name":"stderr"},{"output_type":"stream","text":["checkpoints:  ['outputs']\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:__main__:Loading features from cached file data/cached_dev_bert-base-uncased_128_binary\n","INFO:__main__:***** Running evaluation  *****\n","INFO:__main__:  Num examples = 597\n","INFO:__main__:  Batch size = 8\n"],"name":"stderr"},{"output_type":"stream","text":["eval task:  binary\n","length of final dataset:  597\n","len eval data:  597\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a70e530c639c4bf38d3dfb5166d0590a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(IntProgress(value=0, description='Evaluating', max=75, style=ProgressStyle(description_width='i…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["INFO:__main__:***** Eval results  *****\n","INFO:__main__:  acc = 0.9045226130653267\n"],"name":"stderr"},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"AMb25x63z7Bq","colab":{"base_uri":"https://localhost:8080/","height":33},"outputId":"bd8f17fb-0ebf-4d6a-9981-68410982a4ee","executionInfo":{"status":"ok","timestamp":1576308610536,"user_tz":-330,"elapsed":1315,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}}},"source":["results"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'acc_': 0.9045226130653267}"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"UajIeRe8vALn","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":158},"outputId":"a137bb9b-aff0-4356-d828-3164cf040806","executionInfo":{"status":"error","timestamp":1576308634298,"user_tz":-330,"elapsed":1333,"user":{"displayName":"vaibhav varshney","photoUrl":"","userId":"10158236668516260511"}}},"source":["output"],"execution_count":30,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-30-61353e8b2d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'output' is not defined"]}]},{"cell_type":"code","metadata":{"id":"O5o2AZsTvALr","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}